---
title: "Spatial, Temporal, and Depth Distributions of Mercury Contamination in the Penobscot River, Maine"
author: "Sloane Hauck, Julia Kirkland, Meg O'Brien, and Cira Ortiz"
output: pdf_document
toc: true
lof: true
lot: true
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---
```{r project.setup, results='hide', echo=FALSE, message=FALSE, warning=FALSE, error=FALSE}
packages <- c("tidyverse", "readxl", "dplyr", "lubridate", "sp", "sf","mapview",
              "knitr", "here", "ggplot2", "zoo", "trend")

check.packages <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if(length(new.pkg))
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}
check.packages(packages)

'%ni%' <- Negate('%in%')


#Add theme here:

```

## Background

Enter background text here

## Research Questions and Hypotheses

Enter questions and hypotheses here

## Dataset Information

Summarize where data came from, variables used, etc. 

```{r intro.table.data, echo=FALSE}

#Creates a table summarizing information about the dataset
DatasetInfo <- data.frame(Item = c("Data Source", "Date Range", 
                                   "Number of Records ", "Records with Coordinates"),
                          Value = c("NOAA DIVER", "DATES", 
                                    "28,058", "24,501"))

knitr::kable(DatasetInfo, align = c('l', 'l'))

```

```{r setup.spatial.databases, echo=FALSE}

#Create a point layer of the site coordinates (44.740733, -68.825704)
Lat <- 44.740733
Long <- -68.825704
Name <- "HoltraChem Site"
Site <- data.frame("Site" = Name, "Longitude" = Long, "Latitude" = Lat) %>% 
  st_as_sf(coords=c("Longitude", "Latitude"), crs=4269)

#Penobscot River shapefile
#CSH comment: something insane is happening here
Penobscot <- st_read(here("./Shapefiles/Shedriver.shp")) 
Penobscot_clean <- Penobscot %>% 
  filter(!st_is_empty(geometry)) %>% 
  st_combine() %>% 
  st_make_valid() %>% 
  st_transform(crs=4269)

mapview(Penobscot_clean)

ggplot() +
  geom_sf(data = Penobscot_clean) +
  geom_sf(data = data_wrangle_2)

```

## Data Exploration

Look at what is included in the data and make some initial figures

```{r data.exploration, echo=FALSE}

#Reading in the dataset
data_raw <- read.csv(here("./Raw Data/DIVER_Penobscot_Mercury_2025_11_18_Samples.csv"))

colnames(data_raw)

#Start by subsetting the data to only include columns of interest
#Then we will flag data that has coordinate information
data_wrangle_1 <- data_raw %>% 
  select(Case_Activity, Site_ID, Study_Name, Area_Description, Matrix_Group,
         Station, Date, Sample_ID, Sample_Upper_Depth, Sample_Lower_Depth,
         Sample_Depth_Unit, Depth_Category, Lab_Replicate, ChemCode, Max_Result_.Raw.,
         Analysis_Result_Unit, Start_Latitude, Start_Longitude) %>% 
  mutate(
    Coordinate_Flag = if_else(!is.na(Start_Longitude), "Y", "N"),
    Date = as.Date(Date))

#Here we create a new subset of the data that only includes records with coordinates
#We also make this version of the dataset spatial for future mapping
data_wrangle_2 <- data_wrangle_1 %>% 
  filter(Coordinate_Flag == "Y") %>% 
  filter(Start_Latitude < 44.74788) %>% #this is making sure that we only have data points downstream of the site
  st_as_sf(coords = c("Start_Longitude", "Start_Latitude"),
           crs = 4269)

glimpse(data_wrangle_2)
summary(data_wrangle_2$Max_Result_.Raw.)
#from the summary we see that some results are -9, this is NOAAs method of listing
#denoting that no data is available, so we should exclude it

unique(data_wrangle_2$Analysis_Result_Unit)
#we also see that there are two units, PPM and mg/L. Upon further investigation,
#there is only one record with mg/L, so we will exclude it because that is not a
#valid unit for solids

unique(data_wrangle_2$Sample_Depth_Unit)
#Some of the data has no, or inconsistent depth data, we will need to come up with an average depth value for further analysis

```

Below data exploration/graphing is very preliminary, could use some additional attention to make the figures look nice.

```{r data.exploration.figures, echo=FALSE}
#Plot concentration
ggplot(data_wrangle_2, aes(x=Date, y=Max_Result_.Raw.)) +
  geom_point() +
  labs(y="Concentration (PPM)") +
  geom_hline(yintercept = 1.06, color = "red") #this line represents the mercury PEC, above which harmful effects to sediment dwelling biota are likely to be observed.

#Plot Depths and Concentration
ggplot(data_wrangle_2, aes(x=Date, y=Sample_Upper_Depth, fill=Max_Result_.Raw.)) +
  geom_point(shape=21) +
  scale_y_reverse() +
  labs(y="Depth (cm)", fill="Concentration (PPM)") 


#Map samples to Penobscot River
mapview(Penobscot) + mapview(data_wrangle_2, 
                                                   col.regions = "grey", 
                                                   alpha.regions = 0.5)

```

Based on above discoveries, we need to spatially limit the samples to the river extent, filter out records with -9 as a result, and filter out mg/L units.

## Data Wrangling 

Describe what steps are happening in the data wrangling section below

```{r data.cleaning}

#filter out -9 in results and mg/L units
#also converting sample depths to feet
data_wrangle_3 <- data_wrangle_2 %>% 
  filter(Max_Result_.Raw. > 0) %>% 
  filter(Analysis_Result_Unit == "PPM") %>% 
  mutate(Upper_Depth_ft = if_else(Sample_Upper_Depth >= 0, 
                                  Sample_Upper_Depth*0.0328084,
                                  Sample_Upper_Depth),
         Lower_Depth_ft = if_else(Sample_Lower_Depth >= 0,
                                  Sample_Lower_Depth*0.0328084,
                                  Sample_Lower_Depth),
         Avg_Depth_ft = if_else(Upper_Depth_ft >= 0 & Lower_Depth_ft >= 0,
                               (Upper_Depth_ft+Lower_Depth_ft)/2, -9))

#Intersect the samples with the Penobscot River to ensure we are only looking
#at samples in our area of interest.

data_wrangle_4 <- data_wrangle_3 %>% 
  st_filter(Penobscot) 

#create a summary table

```


Sloane 11/24 - leaving off after doing initial data procesing, wrangling, and some visualization. There are a few spots to fill in (summary tables, figures, etc.), but the bulk of the processing/wrangling should be done!


